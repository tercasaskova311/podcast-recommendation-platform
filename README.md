# Podcast Analytics & Recommendation Platform

This project builds an end-to-end data pipeline to recommend podcast episodes to users based on their interactions and preferences. The workflow includes:

- Downloading new podcast episodes
- Analyzing transcripts
- Tracking user behavior (events)
- Training recommendation models
- Delivering real-time personalized content

---
## Project Structure

root/
â”œâ”€â”€ _delta/ # Delta Lake tables (episodes, events)
â”œâ”€â”€ airflow/ # Apache Airflow config for DAG orchestration
â”œâ”€â”€ config/
â”‚ â””â”€â”€ settings.py # General project settings
â”œâ”€â”€ data/
â”œâ”€â”€ docker/ # Docker-related configuration
â”œâ”€â”€ docs/ # Documentation & diagrams
â”‚ â”œâ”€â”€ architecture.md
â”‚ â”œâ”€â”€ dashboard.md
â”‚ â”œâ”€â”€ datasets.md
â”‚ â”œâ”€â”€ instruction.md
â”‚ â”œâ”€â”€ pipelines.md
â”‚ â””â”€â”€ project_architecture.png
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ batch/
â”‚ â”œâ”€â”€ new_episodes_download.py
â”‚ â””â”€â”€ new_episodes_get_transcripts.py
â”œâ”€â”€ spark/ # Spark pipelines for large-scale processing
â”‚ â”œâ”€â”€ pipelines/
â”‚ â”‚ â”œâ”€â”€ analyze_transcripts_pipeline.py
â”‚ â”‚ â”œâ”€â”€ final_recommendation.py
â”‚ â”‚ â”œâ”€â”€ streaming_user_events_pipeline.py
â”‚ â”‚ â””â”€â”€ training_user_events_pipeline.py
â”‚ â””â”€â”€ util/
â”œâ”€â”€ streaming/ # Real-time stream processing
â”œâ”€â”€ demo/ # Demos or example runs
â”œâ”€â”€ test/ # Unit and integration tests
â”œâ”€â”€ util/ # Utility functions shared across modules
â”œâ”€â”€ dashboard.py # Interactive dashboard
â”œâ”€â”€ .env.development # Environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile # Dev shortcuts (build, test, run)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ðŸ”„ Pipeline Overview

| Stage                          | Description                                                                 |
|-------------------------------|-----------------------------------------------------------------------------|
|  Ingestion                   | Download new episodes, fetch transcripts                                   |
|  Transcript Analysis         | NLP-based feature extraction using Spark                                   |
|  User Events Streaming       | Real-time interaction data (views, clicks, time spent)                     |
|  Model Training              | Collaborative filtering + content-based recommendation models             |
|  Final Recommendation        | Scores and recommends episodes                                             |
|  Dashboard                  | Interactive UI to explore recommendations and model metrics                |

---
##  Technologies

| Area            | Tools Used                           |
|-----------------|--------------------------------------|
| Workflow        | Apache Airflow                       |
| Storage         | Delta Lake (with `_delta` tables),   |
|                 | MongoDB, DuckDB                      |
| Processing      | Apache Spark, Kafka                  |
| Containerization| Docker                               |
| NLP             | Transformers / Text Processing       |
| Real-time       | Spark Streaming                      |
| UI              | Streamlit                            |

---

## Architecture

## Architecture Diagram

![Architecture Diagram](./docs/project_architecture.png)

### Ingest

* Fetch trending episodes from PodcastIndex.com â†’ publish to Kafka (`episode-metadata`, `episode-ids`)
* Sequential downloader/transcriber writes **episodes** & **transcripts** to **Delta**
* User Event simulation - in order to recreate real life data (Stream processing)

### Training / Analytics

* Aggregate **user events** (stream/micro-batch) into **Delta**
* Train **user events** via ALS (Spark ML) to get episode recommendation based on user events

* Batch procesing: Loading transcript text data from Delta and training them 
* Traning transcript data - output: text embedings = text similarities, recomputed each time batch is running 


### Serving

* **MongoDB** stores the `final_recommendations` data which combine similarities recommendation + user event recommendation
* **Streamlit** reads **Mongo** (final_recommendation) + **Delta** (user_events â€œlast N minutesâ€, episodes metadata) via **DuckDB**


---

## Prerequisites

* **Docker Desktop** (with Compose)
* Free local ports for services (Kafka, Mongo, Spark UI, Airflow UI, Streamlit)
* If your network uses a corporate/VPN proxy, ensure Docker can pull from Docker Hub (see **Troubleshooting**)

---

## Instructions to Run the Infrastructure

### Initialize the system

1. **Start Docker**, then from the repo root:

   ```bash
   make init
   ```

   This:

   * builds/starts **Kafka**, **Mongo**, **Airflow** (and Spark build)
   * waits for **Kafka** quorum and **creates topics** (correct partitions/retention)
   * **initializes MongoDB indexes**
   * starts the **Streamlit** dashboard and opens it (if reachable)

2. **Wait for Airflow init container**
   The container `airflow-airflow-init-1` will **exit** after initializing Airflow (expected). All other Airflow containers should be running.

> Prefer manual bring-up?
> `make kafka-up mongo-up airflow-up` â†’ then `make spark-up` and `make dashboard-up`.

---

## Run Demo Data (manual first run)

1. **Load demo Delta** (inside the Airflow scheduler container)

   ```bash
   docker exec -it airflow-airflow-scheduler-1 bash
   cd /opt/project
   python3 scripts/demo/load_delta.py
   ```

2. **Analyze transcripts** (Spark batch)

   ```bash
   python3 spark/pipelines/analyze_transcripts_pipeline.py
   ```

   After success, inspect Mongo (localhost UI if available, or `make mongo-shell`).

3. **Simulate user events** (streaming)

   ```bash
   python3 scripts/streaming/user_events_simulation.py
   ```

   Wait a couple of minutes; you should see `/data/delta/user_vs_episode_daily` populated.

4. **Train similarities & compute final recommendations**

   ```bash
   python3 spark/pipelines/training_user_events_pipeline.py
   python3 spark/pipelines/final_recommendation.py
   ```

   MongoDB should now have **`final_recommendations`** populated.

After these, your **dashboard** can filter/view recommendations + live engagement windows.

---
